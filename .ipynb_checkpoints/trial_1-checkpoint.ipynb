{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0083a555-f6d9-4bbb-a04e-47da1aa37814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d86dc89b-b20c-4009-9c0f-c8349a15cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conjugate Gradient Solver (used in Algorithms 2 and 3) ---\n",
    "def conjugate_gradient(A, b, x0=None, max_iter=200, tol=0.005):\n",
    "    x = x0 if x0 is not None else np.zeros_like(b)\n",
    "    r = b - A @ x\n",
    "    d = r.copy()\n",
    "    for _ in range(max_iter):\n",
    "        Ad = A @ d\n",
    "        alpha = np.dot(r, r) / np.dot(d, Ad)\n",
    "        x = x + alpha * d\n",
    "        r_new = r - alpha * Ad\n",
    "        if np.linalg.norm(r_new) < tol:\n",
    "            break\n",
    "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
    "        d = r_new + beta * d\n",
    "        r = r_new\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d9dcec0-5846-4f1e-858b-8acdcca99b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Algorithm 1: IRLS for RE-WKLR ---\n",
    "def re_wklr_irls(K, y, w1, w0, lam, max_iter=30, dev_thresh=2.5):\n",
    "    n = len(y)\n",
    "    alpha = np.zeros(n)\n",
    "    K_tilda = K + 1e-8 * np.eye(n)  # Regularize K\n",
    "    \n",
    "    dev_old = np.inf\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        eta = K_tilda @ alpha\n",
    "        p = 1 / (1 + np.exp(-eta))\n",
    "\n",
    "        v = p * (1 - p)\n",
    "        w = w1 * y + w0 * (1 - y)\n",
    "        z = eta + (y - p) / (v + 1e-8)\n",
    "\n",
    "        D = np.diag(v * w)\n",
    "        A = K_tilda.T @ D @ K_tilda + lam * K_tilda\n",
    "        b = K_tilda.T @ D @ z\n",
    "        alpha_new = conjugate_gradient(A, b, x0=alpha)\n",
    "\n",
    "        # Bias Correction\n",
    "        Q = np.diag(1 / (v * w + 1e-8))\n",
    "        xi = 0.5 * np.diag(Q) * ((1 + w1) * p - w1)\n",
    "        D_bias = np.diag(v * w)\n",
    "        b_bias = K_tilda.T @ D_bias @ xi\n",
    "        B = conjugate_gradient(A, b_bias)\n",
    "        \n",
    "        alpha_corrected = alpha_new - B\n",
    "        p_corrected = 1 / (1 + np.exp(-(K_tilda @ alpha_corrected)))\n",
    "        dev_new = deviance(y, p_corrected, alpha_corrected, K_tilda, lam)\n",
    "\n",
    "        if abs(dev_old - dev_new) / (dev_new + 1e-8) < dev_thresh:\n",
    "            break\n",
    "        alpha = alpha_new\n",
    "        dev_old = dev_new\n",
    "\n",
    "    return alpha_corrected, p_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a45564-5023-4b13-9411-36d85562ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Bootstrap Hyperparameter Tuning ---\n",
    "# def bootstrap_tuning(X_train, y_train, X_test, y_test, tau, sigma_list, lam_list, B=200):\n",
    "#     best_acc = 0\n",
    "#     best_sigma = None\n",
    "#     best_lam = None\n",
    "\n",
    "#     for sigma in sigma_list:\n",
    "#         K_train = rbf_kernel(X_train, X_train, sigma)\n",
    "#         K_test = rbf_kernel(X_test, X_train, sigma)\n",
    "\n",
    "#         for lam in lam_list:\n",
    "#             acc_class_0 = []\n",
    "#             acc_class_1 = []\n",
    "\n",
    "#             for _ in range(B):\n",
    "#                 X_b, y_b = resample(X_test, y_test, replace=True, stratify=y_test)\n",
    "#                 K_b = rbf_kernel(X_b, X_train, sigma)\n",
    "#                 w1, w0 = compute_weights(y_train, tau)\n",
    "#                 alpha, _ = re_wklr_irls(K_train, y_train, w1, w0, lam)\n",
    "#                 p_b = 1 / (1 + np.exp(-(K_b @ alpha)))\n",
    "#                 y_pred = (p_b > 0.5).astype(int)\n",
    "\n",
    "#                 TP = np.sum((y_b == 1) & (y_pred == 1))\n",
    "#                 FN = np.sum((y_b == 1) & (y_pred == 0))\n",
    "#                 TN = np.sum((y_b == 0) & (y_pred == 0))\n",
    "#                 FP = np.sum((y_b == 0) & (y_pred == 1))\n",
    "\n",
    "#                 acc_1 = TP / (TP + FN + 1e-8)\n",
    "#                 acc_0 = TN / (TN + FP + 1e-8)\n",
    "#                 acc_class_1.append(acc_1)\n",
    "#                 acc_class_0.append(acc_0)\n",
    "\n",
    "#             A = min(np.mean(acc_class_1), np.mean(acc_class_0))\n",
    "#             if A > best_acc:\n",
    "#                 best_acc = A\n",
    "#                 best_sigma = sigma\n",
    "#                 best_lam = lam\n",
    "\n",
    "#     return best_sigma, best_lam, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44cd09e0-7c7e-4541-b91e-1e9113d24e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "ionosphere = fetch_ucirepo(id=52) \n",
    "# liver_disorders = fetch_ucirepo(id=60) \n",
    "# spambase = fetch_ucirepo(id=94) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "\n",
    "X = ionosphere.data.features \n",
    "y = ionosphere.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a4ad2d5a-0bcc-4543-9544-489115e09b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ionosphere.data.features \n",
    "y = ionosphere.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b630ae7-a74b-41ed-b9bd-08ff24023dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sampling_preprocessing_pipeline(X, y, dataset_name=\"general\", test_size=0.2):\n",
    "    \"\"\"Complete sampling and preprocessing pipeline based on the paper's scheme.\"\"\"\n",
    "    \n",
    "    # Ensure X and y are DataFrames\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    if not isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "        y = pd.Series(y)\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.iloc[:, 0]  # Take the first column if y is a DataFrame\n",
    "    \n",
    "    # Normalize X (mean 0, std 1)\n",
    "    scaler = StandardScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Convert categorical y to binary (0 = non-event, 1 = event)\n",
    "    if not np.issubdtype(y.dtype, np.number):\n",
    "        le = LabelEncoder()\n",
    "        y = pd.Series(le.fit_transform(y.values), name='target')\n",
    "    else:\n",
    "        y = pd.Series(y.values, name='target')\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Split classes for sampling\n",
    "    class_0_train = X_train[y_train == 0]\n",
    "    class_1_train = X_train[y_train == 1]\n",
    "    # y_0_train = y_train[y_train == 0]\n",
    "    # y_1_train = y_train[y_train == 1]\n",
    "    \n",
    "    # Balanced and imbalanced training sets\n",
    "    if dataset_name == \"spam\":\n",
    "        X_train_bal = pd.concat([\n",
    "            class_0_train.sample(200, random_state=42),\n",
    "            class_1_train.sample(200, random_state=42)\n",
    "        ])\n",
    "        y_train_bal = y.loc[X_train_bal.index]\n",
    "        \n",
    "        X_train_imb = pd.concat([\n",
    "            class_0_train.sample(200, random_state=42),\n",
    "            class_1_train.sample(100, random_state=42)\n",
    "        ])\n",
    "        y_train_imb = y.loc[X_train_imb.index]\n",
    "    else:\n",
    "        X_train_bal = pd.concat([\n",
    "            class_0_train.sample(40, random_state=42),\n",
    "            class_1_train.sample(40, random_state=42)\n",
    "        ])\n",
    "        y_train_bal = y.loc[X_train_bal.index]\n",
    "        \n",
    "        X_train_imb = pd.concat([\n",
    "            class_0_train.sample(40, random_state=42),\n",
    "            class_1_train.sample(15, random_state=42)\n",
    "        ])\n",
    "        y_train_imb = y.loc[X_train_imb.index]\n",
    "    \n",
    "    # Test set rarity â€” 5% events to non-events ratio (8% for SPECT Heart)\n",
    "    class_0_test = X_test[y_test == 0]\n",
    "    class_1_test = X_test[y_test == 1]\n",
    "    y_0_test = y_test[y_test == 0]\n",
    "    y_1_test = y_test[y_test == 1]\n",
    "    \n",
    "    test_ratio = 0.05 if dataset_name != \"SPECT Heart\" else 0.08\n",
    "    n_test_events = int(round(len(class_0_test) * test_ratio))\n",
    "    \n",
    "    sampled_class_1 = class_1_test.sample(n_test_events, random_state=42)\n",
    "    sampled_y_1 = y_1_test.loc[sampled_class_1.index]\n",
    "    \n",
    "    X_test_final = pd.concat([class_0_test, sampled_class_1])\n",
    "    y_test_final = pd.concat([y_0_test, sampled_y_1])\n",
    "    \n",
    "    # Shuffle X_test_final and y_test_final together\n",
    "    test_final = pd.concat([X_test_final, y_test_final], axis=1).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    X_test_final = test_final.drop(columns=[\"target\"])\n",
    "    y_test_final = test_final[\"target\"]\n",
    "    \n",
    "    return (X_train_bal, y_train_bal), (X_train_imb, y_train_imb), (X_test_final, y_test_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2b4be532-a8c9-464b-a20d-0c0d89667862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training set shape: (80, 34)\n",
      "Imbalanced training set shape: (55, 34)\n",
      "Test set shape: (26, 34)\n"
     ]
    }
   ],
   "source": [
    "(X_train_bal, y_train_bal), (X_train_imb, y_train_imb), (X_test_final, y_test_final) = sampling_preprocessing_pipeline(X, y, dataset_name=\"default\")\n",
    "print(\"Balanced training set shape:\", X_train_bal.shape)\n",
    "print(\"Imbalanced training set shape:\", X_train_imb.shape)\n",
    "print(\"Test set shape:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5f9fc39f-b9cc-432b-871e-d040bae650ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, sigma):\n",
    "    sq_dists = cdist(X1, X2, 'sqeuclidean')\n",
    "    return np.exp(-sq_dists / (2 * sigma ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6d2226e3-b31f-4d9b-9a1c-439a4ba6f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_kernel(K, reg=1e-5):\n",
    "    return K + reg * np.eye(len(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "323a0df6-328b-41e1-989a-911540d251d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviance(y, K, alpha, w):\n",
    "    eta = K @ alpha\n",
    "    p = 1 / (1 + np.exp(-eta))\n",
    "    log_likelihood = np.sum(w * (y * np.log(p) + (1 - y) * np.log(1 - p)))\n",
    "    return -2 * log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fcd0e69a-efbb-4540-8228-4c1afb54f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_solve_alpha(K, D, z, lambda_reg, tol=0.005, max_iter=200):\n",
    "    \"\"\"\n",
    "    Algorithm 2: Linear CG for alpha estimation.\n",
    "    \"\"\"\n",
    "    A = K.T @ D @ K + lambda_reg * K\n",
    "    b = K.T @ D @ z\n",
    "\n",
    "    alpha = np.zeros_like(b)\n",
    "    r = b - A @ alpha\n",
    "    d = r\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        Ad = A @ d\n",
    "        denominator = d.T @ Ad\n",
    "\n",
    "        if np.isnan(denominator) or denominator == 0:\n",
    "            step_size = 0\n",
    "        else:\n",
    "            step_size = (r.T @ r) / denominator\n",
    "\n",
    "        alpha += step_size * d\n",
    "        new_r = r - step_size * Ad\n",
    "\n",
    "        if np.linalg.norm(new_r) < tol:\n",
    "            break\n",
    "\n",
    "        beta = (new_r.T @ new_r) / (r.T @ r)\n",
    "        d = new_r + beta * d\n",
    "        r = new_r\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9237b659-d30a-47a0-8074-b71ba11754b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_solve_bias(K, D, xi, lambda_reg, tol=0.005, max_iter=200):\n",
    "    \"\"\"\n",
    "    Algorithm 3: Linear CG for bias correction.\n",
    "    \"\"\"\n",
    "    A = K.T @ D @ K + lambda_reg * K\n",
    "    b = K.T @ D @ xi\n",
    "\n",
    "    bias = np.zeros_like(b)\n",
    "    r = b - A @ bias\n",
    "    d = r\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        Ad = A @ d\n",
    "        denominator = d.T @ Ad\n",
    "            \n",
    "        if np.isnan(denominator) or denominator == 0:\n",
    "            step_size = 0\n",
    "        else:\n",
    "            step_size = (r.T @ r) / denominator\n",
    "            \n",
    "        # step_size = (r.T @ r) / (d.T @ Ad)\n",
    "        bias += step_size * d\n",
    "        new_r = r - step_size * Ad\n",
    "\n",
    "        if np.linalg.norm(new_r) < tol:\n",
    "            break\n",
    "\n",
    "        beta = (new_r.T @ new_r) / (r.T @ r)\n",
    "        d = new_r + beta * d\n",
    "        r = new_r\n",
    "\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc01e1b9-1035-4ed8-9470-1dc3d736a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WKLR_IRLS(K, y, w1, w0, lambda_reg, max_irls_iter=30, tol=2.5):\n",
    "    \"\"\"\n",
    "    Perform WKLR MLE using Iteratively Reweighted Least Squares (IRLS).\n",
    "    \n",
    "    Parameters:\n",
    "    - K: Regularized kernel matrix\n",
    "    - y: Target labels\n",
    "    - w1, w0: Class weights\n",
    "    - lambda_reg: Regularization parameter lambda\n",
    "    - max_irls_iter: Maximum iterations\n",
    "    - epsilon: Convergence threshold\n",
    "    \n",
    "    Returns:\n",
    "    - Final alpha, bias function B(alpha), unbiased alpha, probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    alpha = np.zeros(K.shape[1])\n",
    "    bias = np.zeros(K.shape[1])\n",
    "\n",
    "    prev_deviance = float('inf')\n",
    "\n",
    "    for _ in range(max_irls_iter):\n",
    "        # Compute probabilities\n",
    "        p_hat = 1 / (1 + np.exp(-K @ alpha))\n",
    "        # print(np.shape(p_hat))\n",
    "        # Compute variance and weights\n",
    "        v = p_hat * (1 - p_hat)\n",
    "        w = w1 * y + w0 * (1 - y)\n",
    "        \n",
    "\n",
    "        # Compute adjusted response\n",
    "        z = K @ alpha + (y - p_hat) / (p_hat * (1 - p_hat))\n",
    "\n",
    "        # Compute weighted logit elements\n",
    "        Q_diag = 1 / (v * w)\n",
    "        # print(f\"Q_diag: {Q_diag}\")\n",
    "        \n",
    "        # Q_diag = np.diag(Q_diag)\n",
    "\n",
    "        # Compute bias response\n",
    "        xi = 0.5 * Q_diag * ((1 + w1) * p_hat - w)\n",
    "\n",
    "        # Compute diagonal weight matrix\n",
    "        D = np.diag(v * w)\n",
    "\n",
    "        alpha = cg_solve_alpha(K, D, z, lambda_reg)\n",
    "\n",
    "        # Solve for bias (Algorithm 3)\n",
    "\n",
    "        bias = cg_solve_bias(K, D, xi, lambda_reg)\n",
    "\n",
    "        # Check convergence\n",
    "        current_deviance = deviance(y, K, alpha, w)\n",
    "        \n",
    "        if abs(prev_deviance - current_deviance)/abs(current_deviance) < tol:\n",
    "            break\n",
    "        prev_deviance = current_deviance\n",
    "    else:\n",
    "        print(\"Algorithm 1 reached maximum iterations\")\n",
    "    \n",
    "    # Compute the unbiased alpha and final probabilities\n",
    "    alpha_corr = alpha - bias\n",
    "    final_p_hat = 1 / (1 + np.exp(-K @ alpha_corr))\n",
    "\n",
    "    # return alpha, bias, alpha_corr, final_p_hat\n",
    "    return alpha_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadf7f1-50e6-45ff-95e4-7feac8791d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_tuning(X_train_, y_train_, X_test_, y_test_, lambda_vals, sigma_vals, dataset_name=\"default\"):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters (Ïƒ, Î») using bootstrap applied to the TEST SET.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import rbf_kernel\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    # Set number of bootstrap rounds (B)\n",
    "    B = 200 if dataset_name.lower() in [\"spam\", \"tornado\"] else 5000\n",
    "\n",
    "    best_acc = 0\n",
    "    best_lambda = None\n",
    "    best_sigma = None\n",
    "\n",
    "    # Loop over Î» and Ïƒ combinations\n",
    "    for lambda_reg, sigma in itertools.product(lambda_vals, sigma_vals):\n",
    "        # print(f\"Testing Î» = {lambda_reg}, Ïƒ = {sigma}\")\n",
    "\n",
    "        # Train the model ONCE on training data\n",
    "        K_train = rbf_kernel(X_train_, X_train_, sigma)\n",
    "        K_train = regularize_kernel(K_train)\n",
    "\n",
    "        # Compute weights for classes\n",
    "        y_bar = np.mean(y_train_)\n",
    "        # tau = 0.8 # user defined\n",
    "        w1 = tau / y_bar\n",
    "        w0 = (1 - tau) / (1 - y_bar)\n",
    "\n",
    "        # Train model on the training data\n",
    "        alpha_corr = WKLR_IRLS(K_train, y_train_, w1, w0, lambda_reg, max_irls_iter=30, tol=2.5)\n",
    "\n",
    "\n",
    "        # Track per-class accuracies for each bootstrap round\n",
    "        class_1_acc = []\n",
    "        class_0_acc = []\n",
    "\n",
    "        \n",
    "        for _ in range(B):\n",
    "            X_sample, y_sample = resample(X_test_, y_test_, replace=True, n_samples=len(X_test_))\n",
    "            \n",
    "            # Compute predictions for the bootstrap sample\n",
    "            K_sample = rbf_kernel(X_sample, X_train_, sigma)\n",
    "\n",
    "            \n",
    "            # y_pred_probs = K_boot @ alpha_boot\n",
    "            # y_pred_boot = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "            y_pred_probs = 1 / (1 + np.exp(-K_sample @ alpha_corr)) \n",
    "            y_pred_sample = (y_pred_probs >= 0.5).astype(int)\n",
    "            # print(y_pred_sample)\n",
    "            \n",
    "            # print('y_pred_sample is:\\n', y_pred_sample)\n",
    "\n",
    "            # Track TP, TN, FP, FN for this bootstrap sample\n",
    "            tn, fp, fn, tp = confusion_matrix(y_sample, y_pred_sample, labels=[0, 1]).ravel()\n",
    "\n",
    "            # Class 1 accuracy (TP rate) & Class 0 accuracy (TN rate)\n",
    "            a1_r = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            a0_r = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "            class_1_acc.append(a1_r)\n",
    "            class_0_acc.append(a0_r)\n",
    "\n",
    "        #  Compute average accuracies per class across all bootstrap samples\n",
    "        a1_avg = np.mean(class_1_acc)\n",
    "        a0_avg = np.mean(class_0_acc)\n",
    "\n",
    "        #  Compute final accuracy for this combination (A = min{a1_avg, a0_avg})\n",
    "        A = min(a1_avg, a0_avg)\n",
    "\n",
    "        # print(f\" Î» = {lambda_reg}, Ïƒ = {sigma}, Final Accuracy A = {A:.4f}\")\n",
    "\n",
    "        # Track the best combination (A* = max{A})\n",
    "        if A > best_acc:\n",
    "            best_acc = A\n",
    "            best_lambda = lambda_reg\n",
    "            best_sigma = sigma\n",
    "\n",
    "    # Return the best combination of (Î», Ïƒ) with max A*\n",
    "    print(\n",
    "        f\"Best combination: Î» = {best_lambda}, Ïƒ = {best_sigma}, Final Accuracy A* = {best_acc:.4f}\"\n",
    "    )\n",
    "    return best_lambda, best_sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b952f4-9859-42b5-955a-df34b4b17d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For tau: 0.1\n",
      "Best combination: Î» = 0.81, Ïƒ = 1.5, Final Accuracy A* = 0.3998\n",
      "For tau: 0.2\n",
      "Best combination: Î» = 0.81, Ïƒ = 1.5, Final Accuracy A* = 0.4001\n",
      "For tau: 0.30000000000000004\n",
      "Best combination: Î» = 0.91, Ïƒ = 1.5, Final Accuracy A* = 0.3999\n",
      "For tau: 0.4\n",
      "Best combination: Î» = 0.81, Ïƒ = 1.5, Final Accuracy A* = 0.4010\n",
      "For tau: 0.5\n"
     ]
    }
   ],
   "source": [
    "sigma_range = np.arange(1, 3.01, 0.5).tolist()\n",
    "lambda_range = np.arange(0.01, 1.01, 0.1).tolist()\n",
    "tauV = np.arange(0.1, 1, 0.1).tolist()\n",
    "\n",
    "for tau in tauV:\n",
    "    print(f\"For tau: {tau}\")\n",
    "    best_sigma, best_lambda = bootstrap_tuning(X_test_final, y_test_final, X_train_bal, y_train_bal, lambda_range, sigma_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc92b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(Î±, y, K, w, Î»):\n",
    "    LL = 0\n",
    "    Î· = K @ Î±\n",
    "    reg = (Î» / 2) * Î±.T @ K @ Î±\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        num = np.exp(y[i] @ Î·[i])\n",
    "        denom = 1 + np.exp(Î·[i])\n",
    "        LL += w[i] * np.log(num / denom)\n",
    "        \n",
    "    # Add regularization term\n",
    "    LL -= reg \n",
    "    return LL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb08e1e1-a0bc-4bfa-be7d-8b1de94452c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviance(Î±, y, K, w, Î»):\n",
    "    return -2 * loglikelihood(Î±, y, K, w, Î»)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4275d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, sigma):\n",
    "    sq_dists = cdist(X1, X2, 'sqeuclidean')\n",
    "    return np.exp(-sq_dists / (2 * sigma ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6353ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_vector(w0, w1, y):\n",
    "    return w1 * y + w0 * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d616032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
